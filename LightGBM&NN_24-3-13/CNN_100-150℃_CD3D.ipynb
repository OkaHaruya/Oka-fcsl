{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce7824c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore', FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eab3a947",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5753432b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tensorflow.keras import models\n",
    "#from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61385086",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input = pd.read_csv(\"CD3D_100-150℃_0.2-1.6mm_0.4-0.9V_std\", usecols=[1,2,3,4,5,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bee2e1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output = pd. read_csv(\"CD3D_100-150℃_0.2-1.6mm_0.4-0.9V\", usecols=[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6aa258fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5940000, 6)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c9360c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5940000, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67c463de",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(df_input, df_output, test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b16ab26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5346000, 6) (594000, 6)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape,x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2036ac2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x座標</th>\n",
       "      <th>y座標</th>\n",
       "      <th>z座標</th>\n",
       "      <th>V[V]</th>\n",
       "      <th>T_cell[℃]</th>\n",
       "      <th>W_ch[mm]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5398873</th>\n",
       "      <td>-0.899747</td>\n",
       "      <td>-1.615496</td>\n",
       "      <td>-0.386727</td>\n",
       "      <td>-9.486833e-01</td>\n",
       "      <td>1.46385</td>\n",
       "      <td>-0.267261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5593541</th>\n",
       "      <td>0.598857</td>\n",
       "      <td>0.209366</td>\n",
       "      <td>-0.088889</td>\n",
       "      <td>1.581139e+00</td>\n",
       "      <td>1.46385</td>\n",
       "      <td>-0.267261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1157183</th>\n",
       "      <td>-1.062096</td>\n",
       "      <td>1.266899</td>\n",
       "      <td>1.298443</td>\n",
       "      <td>-3.162278e-01</td>\n",
       "      <td>-0.87831</td>\n",
       "      <td>-1.069045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4758896</th>\n",
       "      <td>1.214950</td>\n",
       "      <td>0.696630</td>\n",
       "      <td>-1.418201</td>\n",
       "      <td>2.808667e-15</td>\n",
       "      <td>0.87831</td>\n",
       "      <td>1.336306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5713770</th>\n",
       "      <td>1.631228</td>\n",
       "      <td>0.131113</td>\n",
       "      <td>-0.902464</td>\n",
       "      <td>-9.486833e-01</td>\n",
       "      <td>1.46385</td>\n",
       "      <td>1.336306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2249467</th>\n",
       "      <td>-1.145352</td>\n",
       "      <td>-1.211555</td>\n",
       "      <td>1.130829</td>\n",
       "      <td>1.264911e+00</td>\n",
       "      <td>-0.29277</td>\n",
       "      <td>-1.069045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5157699</th>\n",
       "      <td>-0.046376</td>\n",
       "      <td>-1.681710</td>\n",
       "      <td>1.538261</td>\n",
       "      <td>6.324555e-01</td>\n",
       "      <td>1.46385</td>\n",
       "      <td>-1.069045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2215104</th>\n",
       "      <td>-0.895585</td>\n",
       "      <td>-0.443909</td>\n",
       "      <td>1.418352</td>\n",
       "      <td>3.162278e-01</td>\n",
       "      <td>-0.29277</td>\n",
       "      <td>-1.069045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1484405</th>\n",
       "      <td>-0.025562</td>\n",
       "      <td>0.696630</td>\n",
       "      <td>-0.411225</td>\n",
       "      <td>-3.162278e-01</td>\n",
       "      <td>-0.87831</td>\n",
       "      <td>-0.267261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4500015</th>\n",
       "      <td>-0.999654</td>\n",
       "      <td>0.950083</td>\n",
       "      <td>-0.027001</td>\n",
       "      <td>3.162278e-01</td>\n",
       "      <td>0.87831</td>\n",
       "      <td>-0.267261</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5346000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              x座標       y座標       z座標          V[V]  T_cell[℃]  W_ch[mm]\n",
       "5398873 -0.899747 -1.615496 -0.386727 -9.486833e-01    1.46385 -0.267261\n",
       "5593541  0.598857  0.209366 -0.088889  1.581139e+00    1.46385 -0.267261\n",
       "1157183 -1.062096  1.266899  1.298443 -3.162278e-01   -0.87831 -1.069045\n",
       "4758896  1.214950  0.696630 -1.418201  2.808667e-15    0.87831  1.336306\n",
       "5713770  1.631228  0.131113 -0.902464 -9.486833e-01    1.46385  1.336306\n",
       "...           ...       ...       ...           ...        ...       ...\n",
       "2249467 -1.145352 -1.211555  1.130829  1.264911e+00   -0.29277 -1.069045\n",
       "5157699 -0.046376 -1.681710  1.538261  6.324555e-01    1.46385 -1.069045\n",
       "2215104 -0.895585 -0.443909  1.418352  3.162278e-01   -0.29277 -1.069045\n",
       "1484405 -0.025562  0.696630 -0.411225 -3.162278e-01   -0.87831 -0.267261\n",
       "4500015 -0.999654  0.950083 -0.027001  3.162278e-01    0.87831 -0.267261\n",
       "\n",
       "[5346000 rows x 6 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f2b908c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i[A/m^2]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5398873</th>\n",
       "      <td>20750.577957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5593541</th>\n",
       "      <td>2065.708599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1157183</th>\n",
       "      <td>10044.370147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4758896</th>\n",
       "      <td>5115.924705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5713770</th>\n",
       "      <td>8771.056553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2249467</th>\n",
       "      <td>1348.419943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5157699</th>\n",
       "      <td>7995.948169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2215104</th>\n",
       "      <td>6462.545799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1484405</th>\n",
       "      <td>7909.542765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4500015</th>\n",
       "      <td>8490.683815</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5346000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             i[A/m^2]\n",
       "5398873  20750.577957\n",
       "5593541   2065.708599\n",
       "1157183  10044.370147\n",
       "4758896   5115.924705\n",
       "5713770   8771.056553\n",
       "...               ...\n",
       "2249467   1348.419943\n",
       "5157699   7995.948169\n",
       "2215104   6462.545799\n",
       "1484405   7909.542765\n",
       "4500015   8490.683815\n",
       "\n",
       "[5346000 rows x 1 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8453f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras import regularizers, initializers\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "weights = [com.get_weights() for com in model.layers[0:]]  #重さを抽出（※始めに回すときだけ下の行と入れ替える）\n",
    "\n",
    "\n",
    "\n",
    "model.add(Dense(units=32, input_shape=(6,), activation='relu'))  \n",
    "model.add(Dense(units=10, activation='relu'))  #\n",
    "model.add(Dense(units=10, activation='relu'))\n",
    "model.add(Dense(units=1, activation='linear'))  #隠れ層3層でやってみる\n",
    "\n",
    "model.compile(loss='mean_absolute_error',    \n",
    "              optimizer='adam',             #とりあえずadamで\n",
    "              metrics=['mae'],\n",
    "              run_eagerly=False)   #計算時間5倍くらいかかる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "232c27be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20000\n",
      "26730/26730 [==============================] - 35s 1ms/step - loss: 787.7426 - mae: 787.7426 - val_loss: 306.7122 - val_mae: 306.7122 - lr: 0.0010\n",
      "Epoch 2/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 199.3098 - mae: 199.3098 - val_loss: 143.5145 - val_mae: 143.5145 - lr: 0.0010\n",
      "Epoch 3/20000\n",
      "26730/26730 [==============================] - 38s 1ms/step - loss: 134.7582 - mae: 134.7582 - val_loss: 129.1546 - val_mae: 129.1546 - lr: 0.0010\n",
      "Epoch 4/20000\n",
      "26730/26730 [==============================] - 46s 2ms/step - loss: 125.6377 - mae: 125.6377 - val_loss: 122.4915 - val_mae: 122.4915 - lr: 0.0010\n",
      "Epoch 5/20000\n",
      "26730/26730 [==============================] - 38s 1ms/step - loss: 120.8454 - mae: 120.8454 - val_loss: 118.8817 - val_mae: 118.8817 - lr: 0.0010\n",
      "Epoch 6/20000\n",
      "26730/26730 [==============================] - 38s 1ms/step - loss: 118.3426 - mae: 118.3426 - val_loss: 118.6469 - val_mae: 118.6469 - lr: 0.0010\n",
      "Epoch 7/20000\n",
      "26730/26730 [==============================] - 36s 1ms/step - loss: 116.4992 - mae: 116.4992 - val_loss: 115.5067 - val_mae: 115.5067 - lr: 0.0010\n",
      "Epoch 8/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 114.6104 - mae: 114.6104 - val_loss: 112.6789 - val_mae: 112.6789 - lr: 0.0010\n",
      "Epoch 9/20000\n",
      "26730/26730 [==============================] - 36s 1ms/step - loss: 111.6995 - mae: 111.6995 - val_loss: 110.5673 - val_mae: 110.5673 - lr: 0.0010\n",
      "Epoch 10/20000\n",
      "26730/26730 [==============================] - 36s 1ms/step - loss: 107.2807 - mae: 107.2807 - val_loss: 104.4263 - val_mae: 104.4263 - lr: 0.0010\n",
      "Epoch 11/20000\n",
      "26730/26730 [==============================] - 36s 1ms/step - loss: 103.9095 - mae: 103.9095 - val_loss: 103.9192 - val_mae: 103.9192 - lr: 0.0010\n",
      "Epoch 12/20000\n",
      "26730/26730 [==============================] - 36s 1ms/step - loss: 102.8955 - mae: 102.8955 - val_loss: 101.4125 - val_mae: 101.4125 - lr: 0.0010\n",
      "Epoch 13/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 102.2194 - mae: 102.2194 - val_loss: 101.7397 - val_mae: 101.7397 - lr: 0.0010\n",
      "Epoch 14/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 101.6818 - mae: 101.6818 - val_loss: 101.9792 - val_mae: 101.9792 - lr: 0.0010\n",
      "Epoch 15/20000\n",
      "26730/26730 [==============================] - 36s 1ms/step - loss: 101.3115 - mae: 101.3115 - val_loss: 100.4962 - val_mae: 100.4962 - lr: 0.0010\n",
      "Epoch 16/20000\n",
      "26730/26730 [==============================] - 36s 1ms/step - loss: 100.9679 - mae: 100.9679 - val_loss: 101.2726 - val_mae: 101.2726 - lr: 0.0010\n",
      "Epoch 17/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 100.6280 - mae: 100.6280 - val_loss: 100.4830 - val_mae: 100.4830 - lr: 0.0010\n",
      "Epoch 18/20000\n",
      "26730/26730 [==============================] - 36s 1ms/step - loss: 100.2564 - mae: 100.2564 - val_loss: 102.4959 - val_mae: 102.4959 - lr: 0.0010\n",
      "Epoch 19/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 99.8901 - mae: 99.8901 - val_loss: 99.6327 - val_mae: 99.6327 - lr: 0.0010\n",
      "Epoch 20/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 99.4995 - mae: 99.4995 - val_loss: 98.9465 - val_mae: 98.9465 - lr: 0.0010\n",
      "Epoch 21/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 99.1661 - mae: 99.1661 - val_loss: 98.2489 - val_mae: 98.2489 - lr: 0.0010\n",
      "Epoch 22/20000\n",
      "26730/26730 [==============================] - 36s 1ms/step - loss: 98.7286 - mae: 98.7286 - val_loss: 97.8759 - val_mae: 97.8759 - lr: 0.0010\n",
      "Epoch 23/20000\n",
      "26730/26730 [==============================] - 36s 1ms/step - loss: 98.3074 - mae: 98.3074 - val_loss: 97.0734 - val_mae: 97.0734 - lr: 0.0010\n",
      "Epoch 24/20000\n",
      "26730/26730 [==============================] - 35s 1ms/step - loss: 97.8890 - mae: 97.8890 - val_loss: 98.5246 - val_mae: 98.5246 - lr: 0.0010\n",
      "Epoch 25/20000\n",
      "26730/26730 [==============================] - 35s 1ms/step - loss: 97.5324 - mae: 97.5324 - val_loss: 97.2932 - val_mae: 97.2932 - lr: 0.0010\n",
      "Epoch 26/20000\n",
      "26730/26730 [==============================] - 36s 1ms/step - loss: 97.2168 - mae: 97.2168 - val_loss: 96.8129 - val_mae: 96.8129 - lr: 0.0010\n",
      "Epoch 27/20000\n",
      "26730/26730 [==============================] - 36s 1ms/step - loss: 96.9022 - mae: 96.9022 - val_loss: 96.8140 - val_mae: 96.8140 - lr: 0.0010\n",
      "Epoch 28/20000\n",
      "26730/26730 [==============================] - 36s 1ms/step - loss: 96.4300 - mae: 96.4300 - val_loss: 100.2901 - val_mae: 100.2901 - lr: 0.0010\n",
      "Epoch 29/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 95.7061 - mae: 95.7061 - val_loss: 95.1208 - val_mae: 95.1208 - lr: 0.0010\n",
      "Epoch 30/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 95.0321 - mae: 95.0321 - val_loss: 97.4964 - val_mae: 97.4964 - lr: 0.0010\n",
      "Epoch 31/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 94.6470 - mae: 94.6470 - val_loss: 94.8914 - val_mae: 94.8914 - lr: 0.0010\n",
      "Epoch 32/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 94.2562 - mae: 94.2562 - val_loss: 93.4712 - val_mae: 93.4712 - lr: 0.0010\n",
      "Epoch 33/20000\n",
      "26730/26730 [==============================] - 36s 1ms/step - loss: 94.0181 - mae: 94.0181 - val_loss: 94.8193 - val_mae: 94.8193 - lr: 0.0010\n",
      "Epoch 34/20000\n",
      "26730/26730 [==============================] - 35s 1ms/step - loss: 93.8000 - mae: 93.8000 - val_loss: 95.0473 - val_mae: 95.0473 - lr: 0.0010\n",
      "Epoch 35/20000\n",
      "26730/26730 [==============================] - 36s 1ms/step - loss: 93.5918 - mae: 93.5918 - val_loss: 94.8753 - val_mae: 94.8753 - lr: 0.0010\n",
      "Epoch 36/20000\n",
      "26730/26730 [==============================] - 36s 1ms/step - loss: 93.3663 - mae: 93.3663 - val_loss: 93.2674 - val_mae: 93.2674 - lr: 0.0010\n",
      "Epoch 37/20000\n",
      "26730/26730 [==============================] - 36s 1ms/step - loss: 93.2000 - mae: 93.2000 - val_loss: 94.9899 - val_mae: 94.9899 - lr: 0.0010\n",
      "Epoch 38/20000\n",
      "26730/26730 [==============================] - 36s 1ms/step - loss: 92.9645 - mae: 92.9646 - val_loss: 92.4880 - val_mae: 92.4880 - lr: 0.0010\n",
      "Epoch 39/20000\n",
      "26730/26730 [==============================] - 35s 1ms/step - loss: 92.7190 - mae: 92.7190 - val_loss: 92.2615 - val_mae: 92.2615 - lr: 0.0010\n",
      "Epoch 40/20000\n",
      "26730/26730 [==============================] - 36s 1ms/step - loss: 92.4950 - mae: 92.4950 - val_loss: 91.5104 - val_mae: 91.5104 - lr: 0.0010\n",
      "Epoch 41/20000\n",
      "26730/26730 [==============================] - 35s 1ms/step - loss: 92.3095 - mae: 92.3095 - val_loss: 95.5091 - val_mae: 95.5091 - lr: 0.0010\n",
      "Epoch 42/20000\n",
      "26730/26730 [==============================] - 35s 1ms/step - loss: 92.1441 - mae: 92.1441 - val_loss: 91.7923 - val_mae: 91.7923 - lr: 0.0010\n",
      "Epoch 43/20000\n",
      "26730/26730 [==============================] - 35s 1ms/step - loss: 91.9719 - mae: 91.9719 - val_loss: 91.5670 - val_mae: 91.5670 - lr: 0.0010\n",
      "Epoch 44/20000\n",
      "26730/26730 [==============================] - 35s 1ms/step - loss: 91.7761 - mae: 91.7761 - val_loss: 92.3396 - val_mae: 92.3396 - lr: 0.0010\n",
      "Epoch 45/20000\n",
      "26730/26730 [==============================] - 35s 1ms/step - loss: 91.6094 - mae: 91.6094 - val_loss: 93.3517 - val_mae: 93.3517 - lr: 0.0010\n",
      "Epoch 46/20000\n",
      "26730/26730 [==============================] - 35s 1ms/step - loss: 91.4448 - mae: 91.4448 - val_loss: 91.1858 - val_mae: 91.1858 - lr: 0.0010\n",
      "Epoch 47/20000\n",
      "26730/26730 [==============================] - 35s 1ms/step - loss: 91.1987 - mae: 91.1987 - val_loss: 90.4841 - val_mae: 90.4841 - lr: 0.0010\n",
      "Epoch 48/20000\n",
      "26730/26730 [==============================] - 35s 1ms/step - loss: 91.0006 - mae: 91.0006 - val_loss: 90.5559 - val_mae: 90.5559 - lr: 0.0010\n",
      "Epoch 49/20000\n",
      "26730/26730 [==============================] - 36s 1ms/step - loss: 90.7665 - mae: 90.7665 - val_loss: 92.5826 - val_mae: 92.5826 - lr: 0.0010\n",
      "Epoch 50/20000\n",
      "26730/26730 [==============================] - 34s 1ms/step - loss: 90.5767 - mae: 90.5767 - val_loss: 90.1784 - val_mae: 90.1784 - lr: 0.0010\n",
      "Epoch 51/20000\n",
      "26730/26730 [==============================] - 34s 1ms/step - loss: 90.2779 - mae: 90.2779 - val_loss: 91.8489 - val_mae: 91.8489 - lr: 0.0010\n",
      "Epoch 52/20000\n",
      "26730/26730 [==============================] - 34s 1ms/step - loss: 89.9421 - mae: 89.9421 - val_loss: 91.8145 - val_mae: 91.8145 - lr: 0.0010\n",
      "Epoch 53/20000\n",
      "26730/26730 [==============================] - 35s 1ms/step - loss: 89.6564 - mae: 89.6564 - val_loss: 89.1679 - val_mae: 89.1679 - lr: 0.0010\n",
      "Epoch 54/20000\n",
      "26730/26730 [==============================] - 34s 1ms/step - loss: 89.3128 - mae: 89.3128 - val_loss: 88.2244 - val_mae: 88.2244 - lr: 0.0010\n",
      "Epoch 55/20000\n",
      "26730/26730 [==============================] - 35s 1ms/step - loss: 89.0673 - mae: 89.0673 - val_loss: 92.3031 - val_mae: 92.3031 - lr: 0.0010\n",
      "Epoch 56/20000\n",
      "26730/26730 [==============================] - 34s 1ms/step - loss: 88.8045 - mae: 88.8045 - val_loss: 88.7500 - val_mae: 88.7500 - lr: 0.0010\n",
      "Epoch 57/20000\n",
      "26730/26730 [==============================] - 36s 1ms/step - loss: 88.5735 - mae: 88.5735 - val_loss: 89.0740 - val_mae: 89.0740 - lr: 0.0010\n",
      "Epoch 58/20000\n",
      "26730/26730 [==============================] - 36s 1ms/step - loss: 88.2968 - mae: 88.2968 - val_loss: 87.7266 - val_mae: 87.7266 - lr: 0.0010\n",
      "Epoch 59/20000\n",
      "26730/26730 [==============================] - 35s 1ms/step - loss: 88.0976 - mae: 88.0976 - val_loss: 87.7568 - val_mae: 87.7568 - lr: 0.0010\n",
      "Epoch 60/20000\n",
      "26730/26730 [==============================] - 36s 1ms/step - loss: 87.8672 - mae: 87.8672 - val_loss: 90.4229 - val_mae: 90.4229 - lr: 0.0010\n",
      "Epoch 61/20000\n",
      "26730/26730 [==============================] - 36s 1ms/step - loss: 87.6594 - mae: 87.6594 - val_loss: 89.8724 - val_mae: 89.8724 - lr: 0.0010\n",
      "Epoch 62/20000\n",
      "26730/26730 [==============================] - 36s 1ms/step - loss: 87.5256 - mae: 87.5256 - val_loss: 86.8057 - val_mae: 86.8057 - lr: 0.0010\n",
      "Epoch 63/20000\n",
      "26730/26730 [==============================] - 36s 1ms/step - loss: 87.3690 - mae: 87.3690 - val_loss: 86.4154 - val_mae: 86.4154 - lr: 0.0010\n",
      "Epoch 64/20000\n",
      "26730/26730 [==============================] - 36s 1ms/step - loss: 87.2128 - mae: 87.2128 - val_loss: 86.9447 - val_mae: 86.9447 - lr: 0.0010\n",
      "Epoch 65/20000\n",
      "26730/26730 [==============================] - 36s 1ms/step - loss: 87.0801 - mae: 87.0801 - val_loss: 86.2917 - val_mae: 86.2917 - lr: 0.0010\n",
      "Epoch 66/20000\n",
      "26730/26730 [==============================] - 36s 1ms/step - loss: 86.9578 - mae: 86.9578 - val_loss: 86.0707 - val_mae: 86.0707 - lr: 0.0010\n",
      "Epoch 67/20000\n",
      "26730/26730 [==============================] - 36s 1ms/step - loss: 86.8280 - mae: 86.8280 - val_loss: 90.7257 - val_mae: 90.7257 - lr: 0.0010\n",
      "Epoch 68/20000\n",
      "26730/26730 [==============================] - 35s 1ms/step - loss: 86.7626 - mae: 86.7626 - val_loss: 85.7489 - val_mae: 85.7489 - lr: 0.0010\n",
      "Epoch 69/20000\n",
      "26730/26730 [==============================] - 35s 1ms/step - loss: 86.6749 - mae: 86.6749 - val_loss: 85.9025 - val_mae: 85.9025 - lr: 0.0010\n",
      "Epoch 70/20000\n",
      "26730/26730 [==============================] - 35s 1ms/step - loss: 86.5705 - mae: 86.5705 - val_loss: 87.4731 - val_mae: 87.4731 - lr: 0.0010\n",
      "Epoch 71/20000\n",
      "26730/26730 [==============================] - 35s 1ms/step - loss: 86.4726 - mae: 86.4726 - val_loss: 86.8587 - val_mae: 86.8587 - lr: 0.0010\n",
      "Epoch 72/20000\n",
      "26730/26730 [==============================] - 35s 1ms/step - loss: 86.3778 - mae: 86.3778 - val_loss: 85.2574 - val_mae: 85.2574 - lr: 0.0010\n",
      "Epoch 73/20000\n",
      "26730/26730 [==============================] - 35s 1ms/step - loss: 86.3160 - mae: 86.3160 - val_loss: 86.6656 - val_mae: 86.6656 - lr: 0.0010\n",
      "Epoch 74/20000\n",
      "26730/26730 [==============================] - 35s 1ms/step - loss: 86.2229 - mae: 86.2229 - val_loss: 85.5105 - val_mae: 85.5105 - lr: 0.0010\n",
      "Epoch 75/20000\n",
      "26730/26730 [==============================] - 35s 1ms/step - loss: 86.1520 - mae: 86.1520 - val_loss: 87.2294 - val_mae: 87.2294 - lr: 0.0010\n",
      "Epoch 76/20000\n",
      "26730/26730 [==============================] - 35s 1ms/step - loss: 86.0686 - mae: 86.0686 - val_loss: 85.9714 - val_mae: 85.9714 - lr: 0.0010\n",
      "Epoch 77/20000\n",
      "26730/26730 [==============================] - 36s 1ms/step - loss: 85.9675 - mae: 85.9675 - val_loss: 85.3377 - val_mae: 85.3377 - lr: 0.0010\n",
      "Epoch 78/20000\n",
      "26730/26730 [==============================] - 36s 1ms/step - loss: 85.8993 - mae: 85.8993 - val_loss: 85.8822 - val_mae: 85.8822 - lr: 0.0010\n",
      "Epoch 79/20000\n",
      "26730/26730 [==============================] - 35s 1ms/step - loss: 85.8527 - mae: 85.8527 - val_loss: 84.7419 - val_mae: 84.7419 - lr: 0.0010\n",
      "Epoch 80/20000\n",
      "26730/26730 [==============================] - 35s 1ms/step - loss: 85.7626 - mae: 85.7626 - val_loss: 86.1904 - val_mae: 86.1904 - lr: 0.0010\n",
      "Epoch 81/20000\n",
      "26730/26730 [==============================] - 36s 1ms/step - loss: 85.7431 - mae: 85.7431 - val_loss: 85.1360 - val_mae: 85.1360 - lr: 0.0010\n",
      "Epoch 82/20000\n",
      "26730/26730 [==============================] - 36s 1ms/step - loss: 85.6092 - mae: 85.6092 - val_loss: 86.6934 - val_mae: 86.6934 - lr: 0.0010\n",
      "Epoch 83/20000\n",
      "26730/26730 [==============================] - 35s 1ms/step - loss: 85.5932 - mae: 85.5932 - val_loss: 85.0097 - val_mae: 85.0097 - lr: 0.0010\n",
      "Epoch 84/20000\n",
      "26730/26730 [==============================] - 35s 1ms/step - loss: 85.5292 - mae: 85.5292 - val_loss: 85.5355 - val_mae: 85.5355 - lr: 0.0010\n",
      "Epoch 85/20000\n",
      "26730/26730 [==============================] - 35s 1ms/step - loss: 85.4567 - mae: 85.4567 - val_loss: 86.0357 - val_mae: 86.0357 - lr: 0.0010\n",
      "Epoch 86/20000\n",
      "26730/26730 [==============================] - 35s 1ms/step - loss: 85.4247 - mae: 85.4247 - val_loss: 84.4163 - val_mae: 84.4163 - lr: 0.0010\n",
      "Epoch 87/20000\n",
      "26730/26730 [==============================] - 35s 1ms/step - loss: 85.3820 - mae: 85.3820 - val_loss: 86.6749 - val_mae: 86.6749 - lr: 0.0010\n",
      "Epoch 88/20000\n",
      "26730/26730 [==============================] - 36s 1ms/step - loss: 85.3109 - mae: 85.3109 - val_loss: 84.4780 - val_mae: 84.4780 - lr: 0.0010\n",
      "Epoch 89/20000\n",
      "26730/26730 [==============================] - 36s 1ms/step - loss: 85.2419 - mae: 85.2419 - val_loss: 84.5283 - val_mae: 84.5283 - lr: 0.0010\n",
      "Epoch 90/20000\n",
      "26730/26730 [==============================] - 35s 1ms/step - loss: 85.1847 - mae: 85.1847 - val_loss: 84.2272 - val_mae: 84.2272 - lr: 0.0010\n",
      "Epoch 91/20000\n",
      "26730/26730 [==============================] - 36s 1ms/step - loss: 85.1264 - mae: 85.1264 - val_loss: 85.1731 - val_mae: 85.1731 - lr: 0.0010\n",
      "Epoch 92/20000\n",
      "26730/26730 [==============================] - 56s 2ms/step - loss: 85.0832 - mae: 85.0832 - val_loss: 85.1294 - val_mae: 85.1294 - lr: 0.0010\n",
      "Epoch 93/20000\n",
      "26730/26730 [==============================] - 96s 4ms/step - loss: 85.0572 - mae: 85.0572 - val_loss: 84.7947 - val_mae: 84.7947 - lr: 0.0010\n",
      "Epoch 94/20000\n",
      "26730/26730 [==============================] - 83s 3ms/step - loss: 84.9881 - mae: 84.9881 - val_loss: 84.8508 - val_mae: 84.8508 - lr: 0.0010\n",
      "Epoch 95/20000\n",
      "26730/26730 [==============================] - 80s 3ms/step - loss: 84.9322 - mae: 84.9322 - val_loss: 86.5097 - val_mae: 86.5097 - lr: 0.0010\n",
      "Epoch 96/20000\n",
      "26730/26730 [==============================] - 92s 3ms/step - loss: 84.8844 - mae: 84.8844 - val_loss: 84.4222 - val_mae: 84.4222 - lr: 0.0010\n",
      "Epoch 97/20000\n",
      "26730/26730 [==============================] - 85s 3ms/step - loss: 84.8225 - mae: 84.8225 - val_loss: 85.3299 - val_mae: 85.3299 - lr: 0.0010\n",
      "Epoch 98/20000\n",
      "26730/26730 [==============================] - 100s 4ms/step - loss: 84.7240 - mae: 84.7240 - val_loss: 86.8424 - val_mae: 86.8424 - lr: 0.0010\n",
      "Epoch 99/20000\n",
      "26730/26730 [==============================] - 86s 3ms/step - loss: 84.6827 - mae: 84.6827 - val_loss: 84.1597 - val_mae: 84.1597 - lr: 0.0010\n",
      "Epoch 100/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 84.6352 - mae: 84.6352 - val_loss: 84.2326 - val_mae: 84.2326 - lr: 0.0010\n",
      "Epoch 101/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 84.5612 - mae: 84.5612 - val_loss: 84.7874 - val_mae: 84.7874 - lr: 0.0010\n",
      "Epoch 102/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 84.5002 - mae: 84.5002 - val_loss: 85.0134 - val_mae: 85.0134 - lr: 0.0010\n",
      "Epoch 103/20000\n",
      "26730/26730 [==============================] - 38s 1ms/step - loss: 84.4706 - mae: 84.4706 - val_loss: 84.0596 - val_mae: 84.0596 - lr: 0.0010\n",
      "Epoch 104/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 84.4096 - mae: 84.4096 - val_loss: 86.0500 - val_mae: 86.0500 - lr: 0.0010\n",
      "Epoch 105/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 84.3574 - mae: 84.3574 - val_loss: 83.9911 - val_mae: 83.9911 - lr: 0.0010\n",
      "Epoch 106/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 84.3080 - mae: 84.3080 - val_loss: 83.3563 - val_mae: 83.3563 - lr: 0.0010\n",
      "Epoch 107/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 84.2606 - mae: 84.2606 - val_loss: 85.5681 - val_mae: 85.5681 - lr: 0.0010\n",
      "Epoch 108/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 84.2318 - mae: 84.2318 - val_loss: 84.4138 - val_mae: 84.4138 - lr: 0.0010\n",
      "Epoch 109/20000\n",
      "26730/26730 [==============================] - 36s 1ms/step - loss: 84.1658 - mae: 84.1658 - val_loss: 84.3286 - val_mae: 84.3286 - lr: 0.0010\n",
      "Epoch 110/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 84.1189 - mae: 84.1189 - val_loss: 84.1340 - val_mae: 84.1340 - lr: 0.0010\n",
      "Epoch 111/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 84.1297 - mae: 84.1297 - val_loss: 83.5373 - val_mae: 83.5373 - lr: 0.0010\n",
      "Epoch 112/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 84.0559 - mae: 84.0559 - val_loss: 83.7471 - val_mae: 83.7471 - lr: 0.0010\n",
      "Epoch 113/20000\n",
      "26730/26730 [==============================] - 38s 1ms/step - loss: 84.0745 - mae: 84.0745 - val_loss: 84.2054 - val_mae: 84.2054 - lr: 0.0010\n",
      "Epoch 114/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 84.0441 - mae: 84.0441 - val_loss: 83.3347 - val_mae: 83.3347 - lr: 0.0010\n",
      "Epoch 115/20000\n",
      "26730/26730 [==============================] - 38s 1ms/step - loss: 84.0046 - mae: 84.0046 - val_loss: 83.2666 - val_mae: 83.2666 - lr: 0.0010\n",
      "Epoch 116/20000\n",
      "26730/26730 [==============================] - 40s 1ms/step - loss: 83.9306 - mae: 83.9306 - val_loss: 83.3522 - val_mae: 83.3522 - lr: 0.0010\n",
      "Epoch 117/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 83.9192 - mae: 83.9192 - val_loss: 83.5892 - val_mae: 83.5892 - lr: 0.0010\n",
      "Epoch 118/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 83.9118 - mae: 83.9118 - val_loss: 83.3601 - val_mae: 83.3601 - lr: 0.0010\n",
      "Epoch 119/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 83.8838 - mae: 83.8838 - val_loss: 84.2270 - val_mae: 84.2270 - lr: 0.0010\n",
      "Epoch 120/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 83.8502 - mae: 83.8502 - val_loss: 83.0944 - val_mae: 83.0944 - lr: 0.0010\n",
      "Epoch 121/20000\n",
      "26730/26730 [==============================] - 38s 1ms/step - loss: 83.8184 - mae: 83.8184 - val_loss: 84.0814 - val_mae: 84.0814 - lr: 0.0010\n",
      "Epoch 122/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 83.7868 - mae: 83.7868 - val_loss: 84.0339 - val_mae: 84.0339 - lr: 0.0010\n",
      "Epoch 123/20000\n",
      "26730/26730 [==============================] - 38s 1ms/step - loss: 83.7324 - mae: 83.7324 - val_loss: 82.7337 - val_mae: 82.7337 - lr: 0.0010\n",
      "Epoch 124/20000\n",
      "26730/26730 [==============================] - 38s 1ms/step - loss: 83.6888 - mae: 83.6888 - val_loss: 83.3472 - val_mae: 83.3472 - lr: 0.0010\n",
      "Epoch 125/20000\n",
      "26730/26730 [==============================] - 38s 1ms/step - loss: 83.6341 - mae: 83.6341 - val_loss: 82.9696 - val_mae: 82.9696 - lr: 0.0010\n",
      "Epoch 126/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 83.5489 - mae: 83.5489 - val_loss: 85.1429 - val_mae: 85.1429 - lr: 0.0010\n",
      "Epoch 127/20000\n",
      "26730/26730 [==============================] - 38s 1ms/step - loss: 83.4763 - mae: 83.4763 - val_loss: 84.2417 - val_mae: 84.2417 - lr: 0.0010\n",
      "Epoch 128/20000\n",
      "26730/26730 [==============================] - 38s 1ms/step - loss: 83.4485 - mae: 83.4484 - val_loss: 82.9838 - val_mae: 82.9838 - lr: 0.0010\n",
      "Epoch 129/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 83.3350 - mae: 83.3350 - val_loss: 83.1086 - val_mae: 83.1086 - lr: 0.0010\n",
      "Epoch 130/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 83.0546 - mae: 83.0546 - val_loss: 82.0039 - val_mae: 82.0039 - lr: 0.0010\n",
      "Epoch 131/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 82.9390 - mae: 82.9390 - val_loss: 82.9991 - val_mae: 82.9991 - lr: 0.0010\n",
      "Epoch 132/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 82.8456 - mae: 82.8456 - val_loss: 83.3367 - val_mae: 83.3367 - lr: 0.0010\n",
      "Epoch 133/20000\n",
      "26730/26730 [==============================] - 36s 1ms/step - loss: 82.8216 - mae: 82.8216 - val_loss: 85.0912 - val_mae: 85.0912 - lr: 0.0010\n",
      "Epoch 134/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 82.7764 - mae: 82.7764 - val_loss: 83.6443 - val_mae: 83.6443 - lr: 0.0010\n",
      "Epoch 135/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 82.7399 - mae: 82.7399 - val_loss: 82.8014 - val_mae: 82.8014 - lr: 0.0010\n",
      "Epoch 136/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 82.7125 - mae: 82.7125 - val_loss: 83.8312 - val_mae: 83.8312 - lr: 0.0010\n",
      "Epoch 137/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 82.6732 - mae: 82.6732 - val_loss: 82.6807 - val_mae: 82.6807 - lr: 0.0010\n",
      "Epoch 138/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 82.6469 - mae: 82.6469 - val_loss: 82.7718 - val_mae: 82.7718 - lr: 0.0010\n",
      "Epoch 139/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 82.6125 - mae: 82.6125 - val_loss: 83.2812 - val_mae: 83.2812 - lr: 0.0010\n",
      "Epoch 140/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 82.5937 - mae: 82.5937 - val_loss: 82.4008 - val_mae: 82.4008 - lr: 0.0010\n",
      "Epoch 141/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 82.5669 - mae: 82.5669 - val_loss: 82.4238 - val_mae: 82.4238 - lr: 0.0010\n",
      "Epoch 142/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 82.5276 - mae: 82.5276 - val_loss: 83.1743 - val_mae: 83.1743 - lr: 0.0010\n",
      "Epoch 143/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 82.5214 - mae: 82.5214 - val_loss: 82.3899 - val_mae: 82.3899 - lr: 0.0010\n",
      "Epoch 144/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 82.4646 - mae: 82.4646 - val_loss: 82.3565 - val_mae: 82.3565 - lr: 0.0010\n",
      "Epoch 145/20000\n",
      "26730/26730 [==============================] - 38s 1ms/step - loss: 82.4668 - mae: 82.4668 - val_loss: 81.4468 - val_mae: 81.4468 - lr: 0.0010\n",
      "Epoch 146/20000\n",
      "26730/26730 [==============================] - 38s 1ms/step - loss: 82.4404 - mae: 82.4404 - val_loss: 83.4756 - val_mae: 83.4756 - lr: 0.0010\n",
      "Epoch 147/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 82.4080 - mae: 82.4080 - val_loss: 82.9095 - val_mae: 82.9095 - lr: 0.0010\n",
      "Epoch 148/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 82.3744 - mae: 82.3744 - val_loss: 83.3929 - val_mae: 83.3929 - lr: 0.0010\n",
      "Epoch 149/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 82.3649 - mae: 82.3649 - val_loss: 81.4816 - val_mae: 81.4816 - lr: 0.0010\n",
      "Epoch 150/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 82.3171 - mae: 82.3171 - val_loss: 84.7852 - val_mae: 84.7852 - lr: 0.0010\n",
      "Epoch 151/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 82.2807 - mae: 82.2807 - val_loss: 81.5273 - val_mae: 81.5273 - lr: 0.0010\n",
      "Epoch 152/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 82.2891 - mae: 82.2891 - val_loss: 83.3625 - val_mae: 83.3625 - lr: 0.0010\n",
      "Epoch 153/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 82.2488 - mae: 82.2488 - val_loss: 82.6311 - val_mae: 82.6311 - lr: 0.0010\n",
      "Epoch 154/20000\n",
      "26730/26730 [==============================] - 36s 1ms/step - loss: 82.1833 - mae: 82.1833 - val_loss: 82.3434 - val_mae: 82.3434 - lr: 0.0010\n",
      "Epoch 155/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 82.0793 - mae: 82.0793 - val_loss: 83.8503 - val_mae: 83.8503 - lr: 0.0010\n",
      "Epoch 156/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 81.9782 - mae: 81.9782 - val_loss: 82.9499 - val_mae: 82.9499 - lr: 0.0010\n",
      "Epoch 157/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 81.9184 - mae: 81.9184 - val_loss: 81.3601 - val_mae: 81.3601 - lr: 0.0010\n",
      "Epoch 158/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 81.8837 - mae: 81.8837 - val_loss: 82.1496 - val_mae: 82.1496 - lr: 0.0010\n",
      "Epoch 159/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 81.8137 - mae: 81.8137 - val_loss: 81.0677 - val_mae: 81.0677 - lr: 0.0010\n",
      "Epoch 160/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 81.7848 - mae: 81.7848 - val_loss: 81.7100 - val_mae: 81.7100 - lr: 0.0010\n",
      "Epoch 161/20000\n",
      "26730/26730 [==============================] - 38s 1ms/step - loss: 81.7377 - mae: 81.7377 - val_loss: 83.4098 - val_mae: 83.4098 - lr: 0.0010\n",
      "Epoch 162/20000\n",
      "26730/26730 [==============================] - 38s 1ms/step - loss: 81.7394 - mae: 81.7394 - val_loss: 81.1809 - val_mae: 81.1809 - lr: 0.0010\n",
      "Epoch 163/20000\n",
      "26730/26730 [==============================] - 38s 1ms/step - loss: 81.6566 - mae: 81.6566 - val_loss: 84.7564 - val_mae: 84.7564 - lr: 0.0010\n",
      "Epoch 164/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 81.6129 - mae: 81.6128 - val_loss: 82.5700 - val_mae: 82.5700 - lr: 0.0010\n",
      "Epoch 165/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 81.5775 - mae: 81.5775 - val_loss: 81.6231 - val_mae: 81.6231 - lr: 0.0010\n",
      "Epoch 166/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 81.5217 - mae: 81.5217 - val_loss: 81.9815 - val_mae: 81.9815 - lr: 0.0010\n",
      "Epoch 167/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 81.4776 - mae: 81.4776 - val_loss: 81.7416 - val_mae: 81.7416 - lr: 0.0010\n",
      "Epoch 168/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 81.4977 - mae: 81.4977 - val_loss: 80.8033 - val_mae: 80.8033 - lr: 0.0010\n",
      "Epoch 169/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 81.4675 - mae: 81.4675 - val_loss: 85.3516 - val_mae: 85.3516 - lr: 0.0010\n",
      "Epoch 170/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 81.4063 - mae: 81.4063 - val_loss: 82.0558 - val_mae: 82.0558 - lr: 0.0010\n",
      "Epoch 171/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 81.3848 - mae: 81.3848 - val_loss: 81.6799 - val_mae: 81.6799 - lr: 0.0010\n",
      "Epoch 172/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 81.3606 - mae: 81.3606 - val_loss: 82.5937 - val_mae: 82.5937 - lr: 0.0010\n",
      "Epoch 173/20000\n",
      "26730/26730 [==============================] - 36s 1ms/step - loss: 81.2912 - mae: 81.2912 - val_loss: 81.6373 - val_mae: 81.6373 - lr: 0.0010\n",
      "Epoch 174/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 81.2708 - mae: 81.2708 - val_loss: 82.5343 - val_mae: 82.5343 - lr: 0.0010\n",
      "Epoch 175/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 81.2384 - mae: 81.2384 - val_loss: 81.6699 - val_mae: 81.6699 - lr: 0.0010\n",
      "Epoch 176/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 81.2288 - mae: 81.2288 - val_loss: 81.1607 - val_mae: 81.1607 - lr: 0.0010\n",
      "Epoch 177/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 81.1836 - mae: 81.1836 - val_loss: 83.4655 - val_mae: 83.4655 - lr: 0.0010\n",
      "Epoch 178/20000\n",
      "26730/26730 [==============================] - 36s 1ms/step - loss: 81.1440 - mae: 81.1440 - val_loss: 80.6641 - val_mae: 80.6641 - lr: 0.0010\n",
      "Epoch 179/20000\n",
      "26730/26730 [==============================] - 36s 1ms/step - loss: 81.1407 - mae: 81.1407 - val_loss: 80.9180 - val_mae: 80.9180 - lr: 0.0010\n",
      "Epoch 180/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 81.1188 - mae: 81.1188 - val_loss: 86.3910 - val_mae: 86.3910 - lr: 0.0010\n",
      "Epoch 181/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 81.1226 - mae: 81.1226 - val_loss: 81.5774 - val_mae: 81.5774 - lr: 0.0010\n",
      "Epoch 182/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 81.0725 - mae: 81.0725 - val_loss: 80.4984 - val_mae: 80.4984 - lr: 0.0010\n",
      "Epoch 183/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 81.0514 - mae: 81.0514 - val_loss: 81.4974 - val_mae: 81.4974 - lr: 0.0010\n",
      "Epoch 184/20000\n",
      "26730/26730 [==============================] - 38s 1ms/step - loss: 81.0277 - mae: 81.0277 - val_loss: 80.6493 - val_mae: 80.6493 - lr: 0.0010\n",
      "Epoch 185/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 81.0168 - mae: 81.0168 - val_loss: 80.7981 - val_mae: 80.7981 - lr: 0.0010\n",
      "Epoch 186/20000\n",
      "26730/26730 [==============================] - 38s 1ms/step - loss: 81.0086 - mae: 81.0086 - val_loss: 82.5915 - val_mae: 82.5915 - lr: 0.0010\n",
      "Epoch 187/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 80.9788 - mae: 80.9788 - val_loss: 81.4482 - val_mae: 81.4482 - lr: 0.0010\n",
      "Epoch 188/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 80.9507 - mae: 80.9507 - val_loss: 81.9850 - val_mae: 81.9850 - lr: 0.0010\n",
      "Epoch 189/20000\n",
      "26730/26730 [==============================] - 38s 1ms/step - loss: 80.8894 - mae: 80.8894 - val_loss: 80.6562 - val_mae: 80.6562 - lr: 0.0010\n",
      "Epoch 190/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 80.8102 - mae: 80.8102 - val_loss: 80.3916 - val_mae: 80.3916 - lr: 0.0010\n",
      "Epoch 191/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 80.6773 - mae: 80.6773 - val_loss: 80.1898 - val_mae: 80.1898 - lr: 0.0010\n",
      "Epoch 192/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 80.6016 - mae: 80.6016 - val_loss: 83.5130 - val_mae: 83.5130 - lr: 0.0010\n",
      "Epoch 193/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 80.5660 - mae: 80.5660 - val_loss: 83.8593 - val_mae: 83.8593 - lr: 0.0010\n",
      "Epoch 194/20000\n",
      "26730/26730 [==============================] - 36s 1ms/step - loss: 80.4832 - mae: 80.4832 - val_loss: 79.8376 - val_mae: 79.8376 - lr: 0.0010\n",
      "Epoch 195/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 80.4824 - mae: 80.4824 - val_loss: 79.8687 - val_mae: 79.8687 - lr: 0.0010\n",
      "Epoch 196/20000\n",
      "26730/26730 [==============================] - 38s 1ms/step - loss: 80.4586 - mae: 80.4586 - val_loss: 85.9136 - val_mae: 85.9136 - lr: 0.0010\n",
      "Epoch 197/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 80.4153 - mae: 80.4153 - val_loss: 80.1389 - val_mae: 80.1389 - lr: 0.0010\n",
      "Epoch 198/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 80.3961 - mae: 80.3961 - val_loss: 80.0521 - val_mae: 80.0521 - lr: 0.0010\n",
      "Epoch 199/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 80.3389 - mae: 80.3389 - val_loss: 82.1422 - val_mae: 82.1421 - lr: 0.0010\n",
      "Epoch 200/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 80.3154 - mae: 80.3154 - val_loss: 79.8404 - val_mae: 79.8404 - lr: 0.0010\n",
      "Epoch 201/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 80.2857 - mae: 80.2857 - val_loss: 85.1016 - val_mae: 85.1016 - lr: 0.0010\n",
      "Epoch 202/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 80.2720 - mae: 80.2720 - val_loss: 82.9412 - val_mae: 82.9412 - lr: 0.0010\n",
      "Epoch 203/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 80.2255 - mae: 80.2255 - val_loss: 79.0944 - val_mae: 79.0944 - lr: 0.0010\n",
      "Epoch 204/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 80.1911 - mae: 80.1911 - val_loss: 80.2993 - val_mae: 80.2993 - lr: 0.0010\n",
      "Epoch 205/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 79.9273 - mae: 79.9273 - val_loss: 80.5401 - val_mae: 80.5401 - lr: 0.0010\n",
      "Epoch 206/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 79.7973 - mae: 79.7973 - val_loss: 79.2185 - val_mae: 79.2185 - lr: 0.0010\n",
      "Epoch 207/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 79.7784 - mae: 79.7784 - val_loss: 79.4541 - val_mae: 79.4541 - lr: 0.0010\n",
      "Epoch 208/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 79.6979 - mae: 79.6979 - val_loss: 81.0675 - val_mae: 81.0675 - lr: 0.0010\n",
      "Epoch 209/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 79.6551 - mae: 79.6551 - val_loss: 80.3104 - val_mae: 80.3104 - lr: 0.0010\n",
      "Epoch 210/20000\n",
      "26730/26730 [==============================] - 36s 1ms/step - loss: 79.6674 - mae: 79.6674 - val_loss: 79.7928 - val_mae: 79.7928 - lr: 0.0010\n",
      "Epoch 211/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 79.6143 - mae: 79.6143 - val_loss: 79.5521 - val_mae: 79.5521 - lr: 0.0010\n",
      "Epoch 212/20000\n",
      "26730/26730 [==============================] - 36s 1ms/step - loss: 79.5551 - mae: 79.5551 - val_loss: 82.0920 - val_mae: 82.0920 - lr: 0.0010\n",
      "Epoch 213/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 79.5382 - mae: 79.5382 - val_loss: 79.5154 - val_mae: 79.5154 - lr: 0.0010\n",
      "Epoch 214/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 79.5204 - mae: 79.5204 - val_loss: 78.4672 - val_mae: 78.4672 - lr: 0.0010\n",
      "Epoch 215/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 79.4736 - mae: 79.4736 - val_loss: 81.2201 - val_mae: 81.2201 - lr: 0.0010\n",
      "Epoch 216/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 79.4799 - mae: 79.4799 - val_loss: 78.9556 - val_mae: 78.9556 - lr: 0.0010\n",
      "Epoch 217/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 79.4379 - mae: 79.4379 - val_loss: 79.4840 - val_mae: 79.4840 - lr: 0.0010\n",
      "Epoch 218/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 79.4484 - mae: 79.4484 - val_loss: 78.3613 - val_mae: 78.3613 - lr: 0.0010\n",
      "Epoch 219/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 79.3954 - mae: 79.3954 - val_loss: 81.1908 - val_mae: 81.1908 - lr: 0.0010\n",
      "Epoch 220/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 79.3456 - mae: 79.3456 - val_loss: 83.9117 - val_mae: 83.9117 - lr: 0.0010\n",
      "Epoch 221/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 79.3323 - mae: 79.3323 - val_loss: 79.9920 - val_mae: 79.9920 - lr: 0.0010\n",
      "Epoch 222/20000\n",
      "26730/26730 [==============================] - 36s 1ms/step - loss: 79.3478 - mae: 79.3478 - val_loss: 78.8659 - val_mae: 78.8659 - lr: 0.0010\n",
      "Epoch 223/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 79.3141 - mae: 79.3141 - val_loss: 80.7547 - val_mae: 80.7547 - lr: 0.0010\n",
      "Epoch 224/20000\n",
      "26730/26730 [==============================] - 36s 1ms/step - loss: 79.2790 - mae: 79.2790 - val_loss: 78.5344 - val_mae: 78.5344 - lr: 0.0010\n",
      "Epoch 225/20000\n",
      "26730/26730 [==============================] - 36s 1ms/step - loss: 79.2411 - mae: 79.2411 - val_loss: 78.7883 - val_mae: 78.7883 - lr: 0.0010\n",
      "Epoch 226/20000\n",
      "26730/26730 [==============================] - 36s 1ms/step - loss: 79.2331 - mae: 79.2331 - val_loss: 78.8184 - val_mae: 78.8184 - lr: 0.0010\n",
      "Epoch 227/20000\n",
      "26730/26730 [==============================] - 36s 1ms/step - loss: 79.2222 - mae: 79.2222 - val_loss: 81.3106 - val_mae: 81.3106 - lr: 0.0010\n",
      "Epoch 228/20000\n",
      "26730/26730 [==============================] - 36s 1ms/step - loss: 79.2033 - mae: 79.2033 - val_loss: 78.9737 - val_mae: 78.9737 - lr: 0.0010\n",
      "Epoch 229/20000\n",
      "26730/26730 [==============================] - 38s 1ms/step - loss: 79.1822 - mae: 79.1822 - val_loss: 80.2497 - val_mae: 80.2497 - lr: 0.0010\n",
      "Epoch 230/20000\n",
      "26730/26730 [==============================] - 36s 1ms/step - loss: 79.1957 - mae: 79.1957 - val_loss: 79.2892 - val_mae: 79.2892 - lr: 0.0010\n",
      "Epoch 231/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 79.1414 - mae: 79.1414 - val_loss: 81.2647 - val_mae: 81.2647 - lr: 0.0010\n",
      "Epoch 232/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 79.1450 - mae: 79.1450 - val_loss: 79.4899 - val_mae: 79.4899 - lr: 0.0010\n",
      "Epoch 233/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 79.1055 - mae: 79.1055 - val_loss: 79.0100 - val_mae: 79.0100 - lr: 0.0010\n",
      "Epoch 234/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 79.1175 - mae: 79.1175 - val_loss: 80.0997 - val_mae: 80.0997 - lr: 0.0010\n",
      "Epoch 235/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 79.1071 - mae: 79.1071 - val_loss: 79.0871 - val_mae: 79.0871 - lr: 0.0010\n",
      "Epoch 236/20000\n",
      "26730/26730 [==============================] - 36s 1ms/step - loss: 79.0780 - mae: 79.0780 - val_loss: 78.5821 - val_mae: 78.5821 - lr: 0.0010\n",
      "Epoch 237/20000\n",
      "26730/26730 [==============================] - 36s 1ms/step - loss: 79.0927 - mae: 79.0927 - val_loss: 80.7492 - val_mae: 80.7492 - lr: 0.0010\n",
      "Epoch 238/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 79.0723 - mae: 79.0723 - val_loss: 78.3287 - val_mae: 78.3287 - lr: 0.0010\n",
      "Epoch 239/20000\n",
      "26730/26730 [==============================] - 36s 1ms/step - loss: 79.0411 - mae: 79.0411 - val_loss: 78.4728 - val_mae: 78.4728 - lr: 0.0010\n",
      "Epoch 240/20000\n",
      "26730/26730 [==============================] - 36s 1ms/step - loss: 79.0436 - mae: 79.0436 - val_loss: 80.8202 - val_mae: 80.8202 - lr: 0.0010\n",
      "Epoch 241/20000\n",
      "26730/26730 [==============================] - 36s 1ms/step - loss: 79.0453 - mae: 79.0453 - val_loss: 78.1371 - val_mae: 78.1371 - lr: 0.0010\n",
      "Epoch 242/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 79.0137 - mae: 79.0137 - val_loss: 78.7330 - val_mae: 78.7330 - lr: 0.0010\n",
      "Epoch 243/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 78.9845 - mae: 78.9845 - val_loss: 81.3204 - val_mae: 81.3204 - lr: 0.0010\n",
      "Epoch 244/20000\n",
      "26730/26730 [==============================] - 38s 1ms/step - loss: 78.9905 - mae: 78.9905 - val_loss: 79.4560 - val_mae: 79.4560 - lr: 0.0010\n",
      "Epoch 245/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 78.9649 - mae: 78.9649 - val_loss: 78.6942 - val_mae: 78.6942 - lr: 0.0010\n",
      "Epoch 246/20000\n",
      "26730/26730 [==============================] - 38s 1ms/step - loss: 78.9668 - mae: 78.9668 - val_loss: 78.4817 - val_mae: 78.4817 - lr: 0.0010\n",
      "Epoch 247/20000\n",
      "26730/26730 [==============================] - 38s 1ms/step - loss: 78.9487 - mae: 78.9487 - val_loss: 78.6106 - val_mae: 78.6106 - lr: 0.0010\n",
      "Epoch 248/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 78.9555 - mae: 78.9555 - val_loss: 78.2986 - val_mae: 78.2986 - lr: 0.0010\n",
      "Epoch 249/20000\n",
      "26730/26730 [==============================] - 36s 1ms/step - loss: 78.9529 - mae: 78.9529 - val_loss: 79.6364 - val_mae: 79.6364 - lr: 0.0010\n",
      "Epoch 250/20000\n",
      "26730/26730 [==============================] - 38s 1ms/step - loss: 78.9145 - mae: 78.9145 - val_loss: 79.2958 - val_mae: 79.2958 - lr: 0.0010\n",
      "Epoch 251/20000\n",
      "26730/26730 [==============================] - 38s 1ms/step - loss: 78.9204 - mae: 78.9204 - val_loss: 78.9987 - val_mae: 78.9987 - lr: 0.0010\n",
      "Epoch 252/20000\n",
      "26730/26730 [==============================] - 38s 1ms/step - loss: 78.9065 - mae: 78.9065 - val_loss: 78.3780 - val_mae: 78.3780 - lr: 0.0010\n",
      "Epoch 253/20000\n",
      "26730/26730 [==============================] - 38s 1ms/step - loss: 78.8808 - mae: 78.8808 - val_loss: 78.7521 - val_mae: 78.7521 - lr: 0.0010\n",
      "Epoch 254/20000\n",
      "26730/26730 [==============================] - 38s 1ms/step - loss: 78.8916 - mae: 78.8916 - val_loss: 78.2137 - val_mae: 78.2137 - lr: 0.0010\n",
      "Epoch 255/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 78.8741 - mae: 78.8741 - val_loss: 79.7469 - val_mae: 79.7469 - lr: 0.0010\n",
      "Epoch 256/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 78.8686 - mae: 78.8686 - val_loss: 78.9282 - val_mae: 78.9282 - lr: 0.0010\n",
      "Epoch 257/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 78.8677 - mae: 78.8677 - val_loss: 78.8507 - val_mae: 78.8507 - lr: 0.0010\n",
      "Epoch 258/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 78.8475 - mae: 78.8475 - val_loss: 79.7103 - val_mae: 79.7103 - lr: 0.0010\n",
      "Epoch 259/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 78.8346 - mae: 78.8346 - val_loss: 78.3751 - val_mae: 78.3751 - lr: 0.0010\n",
      "Epoch 260/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 78.8436 - mae: 78.8436 - val_loss: 78.6965 - val_mae: 78.6965 - lr: 0.0010\n",
      "Epoch 261/20000\n",
      "26730/26730 [==============================] - 37s 1ms/step - loss: 78.8107 - mae: 78.8107 - val_loss: 78.8720 - val_mae: 78.8720 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "callbacks1 = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_mae',\n",
    "                            factor=0.2,\n",
    "                            patience=5,\n",
    "                            mode=\"auto\",\n",
    "                            min_lr=0.001)\n",
    "callbacks2 = tf.keras.callbacks.EarlyStopping(monitor=\"val_mae\",\n",
    "                                              patience=20,\n",
    "                                              mode=\"auto\")\n",
    "history =  model.fit(x=x_train,\n",
    "                     y=y_train,\n",
    "                     epochs = 20000,\n",
    "                     batch_size=200,\n",
    "                     validation_data=(x_test, y_test),\n",
    "                     callbacks=[callbacks1,callbacks2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd9f2af9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'mae', 'val_loss', 'val_mae', 'lr'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2421dc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c937eb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [com.get_weights() for com in model.layers[0:]] \n",
    "model.layers[0].set_weights(weights[0])\n",
    "model.layers[1].set_weights(weights[1])\n",
    "model.layers[2].set_weights(weights[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce774755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: i_predict CD_3Ddist2\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('i_predict CD_3Ddist2')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
